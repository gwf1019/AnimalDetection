<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.2.313">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>Animal Detection</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1.6em;
  vertical-align: middle;
}
</style>


<script src="AnimalDetection_files/libs/clipboard/clipboard.min.js"></script>
<script src="AnimalDetection_files/libs/quarto-html/quarto.js"></script>
<script src="AnimalDetection_files/libs/quarto-html/popper.min.js"></script>
<script src="AnimalDetection_files/libs/quarto-html/tippy.umd.min.js"></script>
<script src="AnimalDetection_files/libs/quarto-html/anchor.min.js"></script>
<link href="AnimalDetection_files/libs/quarto-html/tippy.css" rel="stylesheet">
<link href="AnimalDetection_files/libs/quarto-html/quarto-syntax-highlighting-dark.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="AnimalDetection_files/libs/bootstrap/bootstrap.min.js"></script>
<link href="AnimalDetection_files/libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="AnimalDetection_files/libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="dark">


</head>

<body class="fullcontent">

<div id="quarto-content" class="page-columns page-rows-contents page-layout-article">

<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title">Animal Detection</h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  

</header>

<p>Presentation by: Chris, Grant, Will</p>
<p><img src="paws.png" height="350"></p>
<section id="background" class="level2">
<h2 class="anchored" data-anchor-id="background">Background</h2>
<p>Image classification is the process of assigning a label or class to an input image based on its visual content. It is a fundamental task in the field of computer vision and has a wide range of applications, including object recognition, face detection, self-driving cars, and medical imaging.</p>
<p>The importance of image classification stems from its ability to enable machines to understand and interpret the visual world in the same way that humans do. This capability can be used to automate various tasks, such as identifying objects in an image, detecting and diagnosing diseases in medical images, and enabling self-driving cars to navigate safely. Additionally, image classification can be used to improve the accuracy of other computer vision tasks, such as object detection and segmentation.</p>
<p>With the increasing amount of visual data being generated and the advancement of deep learning techniques, the field of image classification has seen significant progress in recent years. This has led to the development of highly accurate image classifiers that can be trained on large datasets, and are able to generalize well to new images.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="animals.png" class="img-fluid figure-img"></p>
</figure>
</div>
</section>
<section id="question" class="level2">
<h2 class="anchored" data-anchor-id="question">Question</h2>
<p>Can we build a model that succesfully classifies the images of animals?</p>
</section>
<section id="exploratory-data-analysis" class="level2">
<h2 class="anchored" data-anchor-id="exploratory-data-analysis">Exploratory Data Analysis</h2>
<p>Our data is composed of 7 classes with over 1000 images</p>
<ul>
<li>Dogs</li>
<li>Elephants</li>
<li>Spiders</li>
<li>Chickens</li>
<li>Squirrels</li>
<li>Wolves</li>
<li>Butterflies</li>
<li>Horses</li>
</ul>
<section id="data-cleaning-and-preparing-for-model" class="level3">
<h3 class="anchored" data-anchor-id="data-cleaning-and-preparing-for-model">Data Cleaning and Preparing for Model</h3>
<ul>
<li><p>Each Image was resized to 150x150</p></li>
<li><p>All the images were then given their own label , and converted into an array</p></li>
<li><p>All labels were then one hot encoded.</p></li>
<li><p>We then used a 75% traning set , 15% validation set , and 10% set split.</p></li>
</ul>
</section>
</section>
<section id="methods" class="level2">
<h2 class="anchored" data-anchor-id="methods">Methods</h2>
<p>CNN</p>
<p>A convolutional neural network (CNN) is a type of deep learning model that is commonly used for image and video analysis. It is called “convolutional” because it uses a technique called convolution to analyze the data. In a CNN, the model learns to recognize patterns and features in images by analyzing small sections (or “patches”) of the image at a time, and then combining the information from these patches to understand the overall image. This makes CNNs well-suited for tasks such as image classification, object detection, and image segmentation.</p>
<p>VGG16 model (CNN)</p>
<ul>
<li>13 convolutional layers</li>
<li>5 max pooling layers</li>
<li>3 fully connected layers</li>
<li>GlobalAveragePooling2D() layer</li>
<li>Dropout(0.2) layer</li>
<li>Dense(1024, activation=‘relu’) layer</li>
<li>Dropout(0.2) layer</li>
<li>Dense(6, activation=‘softmax’) layer as the final output layer.</li>
</ul>
<p>So in total, it’s 13 convolutional layers, 5 max pooling layers, 3 dense layers and 2 dropout layers.</p>
<p>The model is trained on the ImageNet dataset, which contains millions of images and thousands of object categories. Because of this, the model has learned rich feature representations for a wide variety of image-based tasks.</p>
<div class="quarto-layout-panel">
<div class="quarto-layout-row quarto-layout-valign-bottom">
<div class="quarto-layout-cell" style="flex-basis: 50.0%;justify-content: center;">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="CNN.png" class="img-fluid figure-img"></p>
<p></p><figcaption class="figure-caption">Convolutional Neural Network (CNN)</figcaption><p></p>
</figure>
</div>
</div>
<div class="quarto-layout-cell" style="flex-basis: 50.0%;justify-content: center;">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="VGG16.png" class="img-fluid figure-img"></p>
<p></p><figcaption class="figure-caption">VGG16 Model</figcaption><p></p>
</figure>
</div>
</div>
</div>
</div>
</section>
<section id="evaluation" class="level2">
<h2 class="anchored" data-anchor-id="evaluation">Evaluation</h2>
<p>As seen in the table below the pretrain VGG model was able to perform significanlty better than our own model. In practice using pretrained models is the way to go and makes life so much easier.</p>
<div class="cell" data-execution_count="1">
<div class="cell-output cell-output-stdout">
<pre><code>        Model  Train Validation   Test
0  Team 6 CNN    99%        40%  32.2%
1       VGG16  97.7%      97.7%  91.2%</code></pre>
</div>
</div>
<p>Reasons our model did not perform well:</p>
<ul>
<li><p>Lack of sufficient data: With a small dataset, the model may learn to memorize the training data, which can lead to overfitting.</p></li>
<li><p>Complex architecture: A complex model with a large number of parameters may have the capacity to memorize the training data, which can lead to overfitting.</p></li>
<li><p>High learning rate: A high learning rate can cause the model to quickly adapt to the training data, which can lead to overfitting if the model does not have time to converge.</p></li>
<li><p>Lack of regularization: Regularization techniques such as dropout, L1, and L2 regularization can help prevent overfitting by adding a penalty to the model for having too many parameters.</p></li>
</ul>
<div class="quarto-layout-panel">
<div class="quarto-layout-row quarto-layout-valign-bottom">
<div class="quarto-layout-cell" style="flex-basis: 50.0%;justify-content: center;">
<p><img src="AccuracyOurs.png" class="img-fluid"></p>
</div>
<div class="quarto-layout-cell" style="flex-basis: 50.0%;justify-content: center;">
<p><img src="LossOurs.png" class="img-fluid"></p>
</div>
</div>
</div>
<p>Reasons VGG16 performs well:</p>
<ul>
<li><p>Deep architecture: VGG16 is a very deep model, with 16 layers, which allows it to learn a complex set of features from the input images. This depth enables the model to extract high-level features from the images, such as edges and shapes, which are important for image classification.</p></li>
<li><p>Small convolutional filters: VGG16 uses small convolutional filters (3x3) which enables the model to learn a large number of feature maps. This makes the model more expressive and able to capture fine-grained details in the images.</p></li>
<li><p>Max pooling: VGG16 uses max pooling layers which reduces the spatial dimension of the feature maps, making the model more robust to small translations of the objects in the images.</p></li>
<li><p>Large number of parameters: VGG16 has a large number of parameters, which enables it to learn a complex set of features from the images.</p></li>
<li><p>Pre-training: VGG16 was pre-trained on a large dataset (ImageNet), which means that it has already learned many useful features that can be used for other tasks.</p></li>
<li><p>ReLU activation: ReLU activation function is used in the model, which allows the network to learn non-linear decision boundaries, which helps to improve the performance of the model. VGG16 just does a much better job in detecting features especially since it has seen millions of similiar photos to ours.</p></li>
</ul>
<p>The metric we will be using to evaluate our model is the accuracy.</p>
<div class="quarto-layout-panel">
<div class="quarto-layout-row quarto-layout-valign-bottom">
<div class="quarto-layout-cell" style="flex-basis: 50.0%;justify-content: center;">
<p><img src="Accuracy.png" class="img-fluid"></p>
</div>
<div class="quarto-layout-cell" style="flex-basis: 50.0%;justify-content: center;">
<p><img src="Loss.png" class="img-fluid"></p>
</div>
</div>
</div>
</section>
<section id="conclusion" class="level2">
<h2 class="anchored" data-anchor-id="conclusion">Conclusion</h2>
<p>We were able to succesfully build a model that classifies animals. We achieved this through a VGG16 model that performed at a 91% accuracy compared to ours that did 30%. This is a 61% difference and 200% increase. Thus our model was two-times better.</p>
<p>CNN models are changing the world in many aspects including:</p>
<ul>
<li><p>Computer Vision: CNNS are used in a wide range of applications such as object detection, image classification, and image segmentation. These models are used in self-driving cars, security systems, and robotics.</p></li>
<li><p>Medical imaging: CNNs are used in medical imaging to improve the accuracy of diagnoses and to detect diseases such as cancer and heart disease.</p></li>
<li><p>Natural Language Processing: CNNs are used in natural language processing (NLP) to analyze text and speech, which has applications in language translation, text summarization, and sentiment analysis.</p></li>
<li><p>Recommender systems: CNNs are used in recommender systems to make personalized recommendations to users based on their past behavior and preferences.</p></li>
<li><p>Generative models: CNNs are used to generate new images, videos and audio, which have applications in art, entertainment, and communication.</p></li>
<li><p>Robotics: CNNs are used in robotics to help the robots understand and interact with their environment, they can be used for tasks such as grasping and manipulation.</p></li>
</ul>
<p>CNNs have the potential to revolutionize many industries and improve the lives of many people by making tasks more efficient, accurate and faster. However, it’s important to keep in mind that these models are not perfect and may have limitations and biases, and it’s important to address these issues to ensure fair and ethical use of these models.</p>
</section>
<section id="sources" class="level2">
<h2 class="anchored" data-anchor-id="sources">Sources</h2>
<p>Very Deep Convolutional Networks for Large-Scale Image Recognition, K. Simonyan and A. Zisserman. arXiv:1409.1556. https://neurohive.io/en/popular-networks/vgg16/ https://arxiv.org/abs/1409.1556 https://www.robots.ox.ac.uk/~vgg/research/very_deep/ https://github.com/keras-team/keras-applications/blob/master/keras_applications/vgg16.py</p>
</section>

</main>
<!-- /main column -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    target: function(trigger) {
      return trigger.previousElementSibling;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->



</body></html>